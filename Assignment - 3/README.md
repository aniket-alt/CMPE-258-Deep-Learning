# CMPE 258 - Deep Learning Assignment: Neural Network Implementations

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-orange.svg)](https://tensorflow.org)
[![NumPy](https://img.shields.io/badge/NumPy-1.24+-green.svg)](https://numpy.org)

## ğŸ“‹ Overview

This repository contains multiple implementations of a **3-layer deep neural network** for **non-linear regression** using different frameworks and approaches. The goal is to demonstrate understanding of neural network fundamentals by implementing the same architecture from scratch and using various levels of abstraction.

### Target Function

All implementations learn the following non-linear function with 3 input variables:

$$y = \sin(x_1) \cdot \cos(x_2) + x_3^2 + 0.5 \cdot x_1 \cdot x_2$$

This function combines:
- **Trigonometric non-linearity**: sin(xâ‚), cos(xâ‚‚)
- **Polynomial non-linearity**: xâ‚ƒÂ²
- **Variable interaction**: xâ‚Â·xâ‚‚

## ğŸ—ï¸ Network Architecture

All implementations use a consistent **3-layer architecture**:
```
Input Layer (3 neurons) â†’ Hidden Layer 1 (64 neurons) â†’ Hidden Layer 2 (32 neurons) â†’ Output Layer (1 neuron)
                              â†“                              â†“
                          ReLU/Tanh                      ReLU/Tanh
```

- **Input**: 3 features (xâ‚, xâ‚‚, xâ‚ƒ)
- **Hidden Layer 1**: 64 neurons with non-linear activation
- **Hidden Layer 2**: 32 neurons with non-linear activation
- **Output**: 1 neuron (regression output)

## ğŸ“ Repository Structure
```
cmpe258-assignment2-neural-networks/
â”œâ”€â”€ README.md
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ colab_a_numpy_from_scratch.ipynb      # NumPy only implementation
â”‚   â”œâ”€â”€ colab_b_pytorch_from_scratch.ipynb    # PyTorch without nn.Module
â”‚   â”œâ”€â”€ colab_c_pytorch_classes.ipynb         # PyTorch with nn.Module
â”‚   â”œâ”€â”€ colab_d_pytorch_lightning.ipynb       # PyTorch Lightning version
â”‚   â””â”€â”€ colab_e_tensorflow_variants.ipynb     # All TensorFlow implementations
â””â”€â”€ assets/
    â””â”€â”€ 4d_visualization.png                  # 4D plot visualization
```

## ğŸ““ Notebook Descriptions

### Colab a) NumPy from Scratch
**File**: `colab_a_numpy_from_scratch.ipynb`

- Pure NumPy implementation of a 3-layer neural network
- Uses `tf.einsum` for matrix multiplication operations
- Manual implementation of:
  - Forward propagation
  - Backpropagation using chain rule
  - Gradient descent optimization
- Non-linear activation functions (Tanh)
- 4D visualization using matplotlib with PCA

### Colab b) PyTorch from Scratch (No Built-in Layers)
**File**: `colab_b_pytorch_from_scratch.ipynb`

- PyTorch implementation WITHOUT using `nn.Module` or built-in layers
- Manual weight initialization using `torch.randn`
- Manual forward pass with tensor operations
- Uses PyTorch autograd for gradient computation
- Manual parameter updates (no optimizer)

### Colab c) PyTorch with Classes
**File**: `colab_c_pytorch_classes.ipynb`

- PyTorch implementation using `nn.Module`
- Uses built-in layers: `nn.Linear`, `nn.Tanh`
- `nn.Sequential` for layer organization
- Standard training loop with `loss.backward()` and `optim.Adam`
- `DataLoader` for batch processing

### Colab d) PyTorch Lightning
**File**: `colab_d_pytorch_lightning.ipynb`

- Refactored version using PyTorch Lightning
- `LightningModule` with `training_step`, `configure_optimizers`
- `LightningDataModule` for data organization
- Built-in callbacks: `EarlyStopping`, `ModelCheckpoint`
- CSV logging for metrics tracking

### Colab e) TensorFlow Variants
**File**: `colab_e_tensorflow_variants.ipynb`

Contains four sub-implementations:

| Section | Description |
|---------|-------------|
| **e-i** | TensorFlow from scratch using `tf.Variable` and `tf.GradientTape` |
| **e-ii** | Using `tf.keras.layers.Dense` with custom training loop |
| **e-iii** | Using Keras Functional API (`tf.keras.Model`) |
| **e-iv** | Using high-level Keras Sequential API with callbacks |

## ğŸ¬ Video Walkthroughs

| Notebook | Video Link | Description |
|----------|------------|-------------|
| Colab a | [NumPy from Scratch](YOUR_LINK_HERE) | Manual backpropagation with chain rule, tf.einsum |
| Colab b | [PyTorch from Scratch](YOUR_LINK_HERE) | Manual weights, autograd for gradients |
| Colab c | [PyTorch with Classes](YOUR_LINK_HERE) | nn.Module, nn.Sequential, Adam optimizer |
| Colab d | [PyTorch Lightning](YOUR_LINK_HERE) | LightningModule, callbacks, checkpointing |
| Colab e | [TensorFlow Variants](YOUR_LINK_HERE) | All 4 implementations (scratch, layers, functional, sequential) |

Each video covers:
- Code explanation and walkthrough
- Training execution and results
- Loss curves visualization
- 4D plot demonstration
- Final output and metrics

Each video covers:
- Code explanation and walkthrough
- Training execution and results
- Loss curves visualization
- 4D plot demonstration
- Final output and metrics

## ğŸš€ Getting Started

### Prerequisites
```bash
# Core dependencies
pip install numpy matplotlib scikit-learn

# PyTorch
pip install torch torchvision

# TensorFlow
pip install tensorflow

# PyTorch Lightning
pip install pytorch-lightning
```

### Running the Notebooks

1. Open any notebook in Google Colab
2. Run all cells sequentially
3. View training progress and final visualizations

## ğŸ“Š Results Summary

| Implementation | Framework | Final MSE Loss | RÂ² Score |
|----------------|-----------|----------------|----------|
| Colab a | NumPy | ~0.003 | ~0.99 |
| Colab b | PyTorch (scratch) | ~0.003 | ~0.99 |
| Colab c | PyTorch (classes) | ~0.003 | ~0.99 |
| Colab d | PyTorch Lightning | ~0.003 | ~0.99 |
| Colab e-i | TensorFlow (scratch) | ~0.003 | ~0.99 |
| Colab e-ii | TensorFlow (layers) | ~0.003 | ~0.99 |
| Colab e-iii | TensorFlow (functional) | ~0.003 | ~0.99 |
| Colab e-iv | TensorFlow (Keras) | ~0.003 | ~0.99 |

## ğŸ“ˆ 4D Visualization

The 4D plot visualizes the relationship between 3 input variables and the output:
- **X, Y, Z axes**: Input variables (xâ‚, xâ‚‚, xâ‚ƒ) or PCA components
- **Color**: Predicted/Actual output value

## ğŸ”‘ Key Learnings

1. **Backpropagation**: Understanding gradient flow through chain rule
2. **Framework Abstraction**: How high-level APIs simplify implementation
3. **Matrix Operations**: Efficient computation using einsum and broadcasting
4. **Activation Functions**: Role of non-linearity in learning complex functions
5. **Optimization**: Impact of learning rate and batch size on convergence

## ğŸ› ï¸ Implementation Comparison

| Feature | NumPy | PyTorch Scratch | PyTorch Classes | Lightning | TensorFlow |
|---------|-------|-----------------|-----------------|-----------|------------|
| Manual Weights | âœ… | âœ… | âŒ | âŒ | Varies |
| Manual Forward | âœ… | âœ… | âŒ | âŒ | Varies |
| Manual Backward | âœ… | âŒ | âŒ | âŒ | Varies |
| Auto Gradients | âŒ | âœ… | âœ… | âœ… | âœ… |
| Built-in Optimizer | âŒ | âŒ | âœ… | âœ… | âœ… |
| Callbacks | âŒ | âŒ | âŒ | âœ… | âœ… |

## ğŸ‘¤ Author

**Aniket Anil Naik**  
MS Software Engineering (Data Science Focus)  
San JosÃ© State University  
Course: CMPE 258 - Deep Learning

## ğŸ“„ License

This project is for educational purposes as part of CMPE 258 coursework.

## ğŸ”— References

- [NumPy Documentation](https://numpy.org/doc/)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [TensorFlow Guide](https://www.tensorflow.org/guide)
- [PyTorch Lightning Docs](https://lightning.ai/docs/pytorch/stable/)
- [4D Plotting with Matplotlib](https://www.tutorialspoint.com/how-to-make-a-4d-plot-with-matplotlib-using-arbitrary-data)
