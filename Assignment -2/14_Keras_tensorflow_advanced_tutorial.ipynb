{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniket-alt/CMPE-258-Deep-Learning/blob/main/Assignment%20-2/14_Keras_tensorflow_advanced_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEkrSo01WcC7"
      },
      "source": [
        "# TensorFlow & Keras Advanced: Custom Layers and Deep Architectures\n",
        "\n",
        "## Part 2: Building Complex Operations from Scratch\n",
        "\n",
        "---\n",
        "\n",
        "In Part 1, we learned TensorFlow fundamentals, GradientTape basics, and the high-level Keras API. Now we go deeper!\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "| Part | Topic | Key Concepts |\n",
        "|------|-------|-------------|\n",
        "| **I** | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| **II** | Building Ops from Scratch | Convolution, pooling, normalization by hand |\n",
        "| **III** | Custom Layers (Primitives) | Build layers using only tf.Variable |\n",
        "| **IV** | Custom Keras Layers | Proper subclassing with `build()` and `call()` |\n",
        "| **V** | Advanced Architectures | Residual blocks, attention, custom normalizations |\n",
        "| **VI** | Custom Training Loops | Full control over training with GradientTape |\n",
        "| **VII** | Practical Demos | Real-world examples with custom components |\n",
        "\n",
        "---\n",
        "\n",
        "*\"To understand the framework, build it from scratch.\"*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQaXbL0qWcC7",
        "outputId": "30b742da-ffcd-4308-aa8c-4844ce3ffb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Keras Version:      3.10.0\n",
            "GPU Available:      False\n",
            "\n",
            "Ready for Advanced TensorFlow & Keras!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                           SETUP & IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, Model, Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional, Callable, Union\n",
        "\n",
        "# Beautiful plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Check versions\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"Keras Version:      {keras.__version__}\")\n",
        "print(f\"GPU Available:      {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# GPU memory growth\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\\nReady for Advanced TensorFlow & Keras!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tItbO-4WcC8"
      },
      "source": [
        "---\n",
        "\n",
        "# Part I: Advanced GradientTape Patterns\n",
        "\n",
        "## Beyond Basic Gradient Computation\n",
        "\n",
        "In Part 1, we used GradientTape for simple gradients. Now we'll explore:\n",
        "\n",
        "- **Nested tapes** for higher-order derivatives\n",
        "- **Jacobian and Hessian** computation\n",
        "- **Custom gradients** for non-differentiable operations\n",
        "- **Gradient clipping** and manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9-VhzBFWcC8",
        "outputId": "fdd40db7-cd8e-40e2-bcfc-c9358f027dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       NESTED TAPES: HIGHER-ORDER DERIVATIVES\n",
            "============================================================\n",
            "\n",
            "f(x) = x^4, evaluated at x = 2.0\n",
            "\n",
            "f(x)    = 16.0         (expected: 16)\n",
            "f'(x)   = 32.0         (expected: 32 = 4*8)\n",
            "f''(x)  = 48.0         (expected: 48 = 12*4)\n",
            "f'''(x) = 48.0         (expected: 48 = 24*2)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    NESTED GRADIENTTAPES: HIGHER-ORDER DERIVATIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       NESTED TAPES: HIGHER-ORDER DERIVATIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Example: Compute first, second, and third derivatives\n",
        "# f(x) = x^4\n",
        "# f'(x) = 4x^3\n",
        "# f''(x) = 12x^2\n",
        "# f'''(x) = 24x\n",
        "\n",
        "x = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape() as tape3:\n",
        "    with tf.GradientTape() as tape2:\n",
        "        with tf.GradientTape() as tape1:\n",
        "            y = x ** 4\n",
        "        dy_dx = tape1.gradient(y, x)      # First derivative: 4x^3\n",
        "    d2y_dx2 = tape2.gradient(dy_dx, x)    # Second derivative: 12x^2\n",
        "d3y_dx3 = tape3.gradient(d2y_dx2, x)      # Third derivative: 24x\n",
        "\n",
        "print(f\"\\nf(x) = x^4, evaluated at x = {x.numpy()}\")\n",
        "print(f\"\")\n",
        "print(f\"f(x)    = {y.numpy():.1f}         (expected: 16)\")\n",
        "print(f\"f'(x)   = {dy_dx.numpy():.1f}         (expected: 32 = 4*8)\")\n",
        "print(f\"f''(x)  = {d2y_dx2.numpy():.1f}         (expected: 48 = 12*4)\")\n",
        "print(f\"f'''(x) = {d3y_dx3.numpy():.1f}         (expected: 48 = 24*2)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdSJanpUWcC8",
        "outputId": "b7325010-df9d-42bb-98db-f79572c726ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              JACOBIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "x = [1. 2. 3.]\n",
            "y = f(x) = [x1^2, x1*x2, sin(x3)] = [1.      2.      0.14112]\n",
            "\n",
            "Jacobian (3x3):\n",
            "[[ 2.         0.         0.       ]\n",
            " [ 2.         1.         0.       ]\n",
            " [ 0.         0.        -0.9899925]]\n",
            "\n",
            "Expected Jacobian:\n",
            "  [2*x1,   0,      0   ]   = [2,   0,     0     ]\n",
            "  [x2,     x1,     0   ]   = [2,   1,     0     ]\n",
            "  [0,      0,  cos(x3) ]   = [0,   0,  -0.9900]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    JACOBIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              JACOBIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Jacobian is the matrix of all first-order partial derivatives\n",
        "# For f: R^n -> R^m, the Jacobian J is m x n where J[i,j] = df_i/dx_j\n",
        "\n",
        "x = tf.Variable([1.0, 2.0, 3.0])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    # Vector function: f(x) = [x1^2, x1*x2, sin(x3)]\n",
        "    y = tf.stack([\n",
        "        x[0] ** 2,\n",
        "        x[0] * x[1],\n",
        "        tf.sin(x[2])\n",
        "    ])\n",
        "\n",
        "# Compute full Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "print(f\"\\nx = {x.numpy()}\")\n",
        "print(f\"y = f(x) = [x1^2, x1*x2, sin(x3)] = {y.numpy()}\")\n",
        "print(f\"\\nJacobian (3x3):\")\n",
        "print(f\"{jacobian.numpy()}\")\n",
        "\n",
        "print(f\"\\nExpected Jacobian:\")\n",
        "print(f\"  [2*x1,   0,      0   ]   = [2,   0,     0     ]\")\n",
        "print(f\"  [x2,     x1,     0   ]   = [2,   1,     0     ]\")\n",
        "print(f\"  [0,      0,  cos(x3) ]   = [0,   0,  {np.cos(3):.4f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_a3dkRlWcC8",
        "outputId": "d14508f9-431c-40bc-d7a4-560e219fc628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              HESSIAN COMPUTATION\n",
            "============================================================\n",
            "\n",
            "f(x, y) = x^2*y + y^3, at (x, y) = (1.0, 2.0)\n",
            "f = 10.0\n",
            "\n",
            "Gradient: [ 4. 13.]\n",
            "  Expected: [2xy, x^2 + 3y^2] = [4, 13]\n",
            "\n",
            "Hessian:\n",
            "[[ 4.  2.]\n",
            " [ 2. 12.]]\n",
            "  Expected:\n",
            "  [2y,  2x ]   = [4, 2]\n",
            "  [2x,  6y ]   = [2, 12]\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    HESSIAN COMPUTATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              HESSIAN COMPUTATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The Hessian is the matrix of second-order partial derivatives\n",
        "# H[i,j] = d^2f / (dx_i dx_j)\n",
        "\n",
        "x = tf.Variable([1.0, 2.0])\n",
        "\n",
        "with tf.GradientTape() as tape2:\n",
        "    with tf.GradientTape() as tape1:\n",
        "        # Scalar function: f(x, y) = x^2*y + y^3\n",
        "        f = x[0]**2 * x[1] + x[1]**3\n",
        "    grad = tape1.gradient(f, x)  # [2xy, x^2 + 3y^2]\n",
        "hessian = tape2.jacobian(grad, x)\n",
        "\n",
        "print(f\"\\nf(x, y) = x^2*y + y^3, at (x, y) = ({x[0].numpy()}, {x[1].numpy()})\")\n",
        "print(f\"f = {f.numpy()}\")\n",
        "print(f\"\\nGradient: {grad.numpy()}\")\n",
        "print(f\"  Expected: [2xy, x^2 + 3y^2] = [4, 13]\")\n",
        "print(f\"\\nHessian:\")\n",
        "print(f\"{hessian.numpy()}\")\n",
        "print(f\"  Expected:\")\n",
        "print(f\"  [2y,  2x ]   = [4, 2]\")\n",
        "print(f\"  [2x,  6y ]   = [2, 12]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srxJFP_KWcC9",
        "outputId": "28452556-c47b-49e8-e985-abb393ca6470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "              CUSTOM GRADIENTS\n",
            "============================================================\n",
            "\n",
            "Input: [3. 4.]\n",
            "Gradient (clipped to norm 1.0): [0.70710677 0.70710677]\n",
            "Gradient norm: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM GRADIENTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"              CUSTOM GRADIENTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sometimes you need to define custom gradients:\n",
        "# - For non-differentiable operations (like argmax)\n",
        "# - For numerical stability\n",
        "# - For custom backward passes (like straight-through estimators)\n",
        "\n",
        "@tf.custom_gradient\n",
        "def clip_gradient_norm(x, clip_value=1.0):\n",
        "    \"\"\"\n",
        "    Forward: identity function\n",
        "    Backward: clip gradient norm\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Clip the incoming gradient\n",
        "        norm = tf.norm(dy)\n",
        "        clipped = tf.cond(\n",
        "            norm > clip_value,\n",
        "            lambda: dy * clip_value / norm,\n",
        "            lambda: dy\n",
        "        )\n",
        "        return clipped  # Only return gradient for 'x'\n",
        "    return x, grad\n",
        "\n",
        "# Test custom gradient\n",
        "x = tf.Variable([3.0, 4.0])  # Gradient will have norm 5 (3-4-5 triangle)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = clip_gradient_norm(x, clip_value=1.0)\n",
        "    loss = tf.reduce_sum(y)  # Gradient would be [1, 1] but we pass [3, 4]\n",
        "\n",
        "# Manually set upstream gradient to [3, 4]\n",
        "grad = tape.gradient(loss, x)\n",
        "print(f\"\\nInput: {x.numpy()}\")\n",
        "print(f\"Gradient (clipped to norm 1.0): {grad.numpy()}\")\n",
        "print(f\"Gradient norm: {tf.norm(grad).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8-8EpEWcC9",
        "outputId": "3c064a91-4b41-49b5-f043-8116d8e92ec8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         STRAIGHT-THROUGH ESTIMATOR\n",
            "============================================================\n",
            "\n",
            "Input:   [0.3 0.7 1.2 2.5]\n",
            "Rounded: [0. 1. 1. 2.]\n",
            "Gradient (straight-through): [0. 2. 2. 4.]\n",
            "\n",
            " Note: Round is non-differentiable, but we can still train!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    STRAIGHT-THROUGH ESTIMATOR\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         STRAIGHT-THROUGH ESTIMATOR\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# The straight-through estimator is used for:\n",
        "# - Binary/discrete operations that are non-differentiable\n",
        "# - Quantization in neural networks\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_round(x):\n",
        "    \"\"\"\n",
        "    Forward: round to nearest integer\n",
        "    Backward: pass gradient through unchanged (identity)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        return dy  # Straight-through: gradient = identity\n",
        "    return tf.round(x), grad\n",
        "\n",
        "@tf.custom_gradient\n",
        "def straight_through_sign(x):\n",
        "    \"\"\"\n",
        "    Forward: sign function (-1, 0, or 1)\n",
        "    Backward: gradient of hard tanh (1 if |x| <= 1, else 0)\n",
        "    \"\"\"\n",
        "    def grad(dy):\n",
        "        # Gradient is 1 where |x| <= 1, 0 elsewhere\n",
        "        return dy * tf.cast(tf.abs(x) <= 1, dy.dtype)\n",
        "    return tf.sign(x), grad\n",
        "\n",
        "# Test\n",
        "x = tf.Variable([0.3, 0.7, 1.2, 2.5])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    y = straight_through_round(x)\n",
        "    loss = tf.reduce_sum(y ** 2)\n",
        "\n",
        "grad = tape.gradient(loss, x)\n",
        "\n",
        "print(f\"\\nInput:   {x.numpy()}\")\n",
        "print(f\"Rounded: {y.numpy()}\")\n",
        "print(f\"Gradient (straight-through): {grad.numpy()}\")\n",
        "print(f\"\\n Note: Round is non-differentiable, but we can still train!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uwxn2-LWcC9",
        "outputId": "a317f145-6a31-49cf-8c02-479304ab3479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            GRADIENT ACCUMULATION\n",
            "============================================================\n",
            "\n",
            "Gradient Accumulation Pattern:\n",
            "  1. Compute gradients for mini-batch\n",
            "  2. Accumulate (sum or average) over N steps\n",
            "  3. Apply accumulated gradients once\n",
            "  4. Effective batch = mini_batch * N\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    GRADIENT ACCUMULATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            GRADIENT ACCUMULATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gradient accumulation is useful when:\n",
        "# - Batch size is too large for GPU memory\n",
        "# - You want effective larger batch sizes\n",
        "\n",
        "def train_with_accumulation(model, data, labels, batch_size, accumulation_steps, optimizer):\n",
        "    \"\"\"\n",
        "    Train with gradient accumulation.\n",
        "    Effective batch size = batch_size * accumulation_steps\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    accumulated_gradients = [tf.zeros_like(v) for v in model.trainable_variables]\n",
        "\n",
        "    for step in range(accumulation_steps):\n",
        "        # Get mini-batch\n",
        "        start = (step * batch_size) % n_samples\n",
        "        end = start + batch_size\n",
        "        x_batch = data[start:end]\n",
        "        y_batch = labels[start:end]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(x_batch, training=True)\n",
        "            loss = tf.reduce_mean(keras.losses.mse(y_batch, predictions))\n",
        "\n",
        "        # Compute gradients\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "        # Accumulate (average over steps)\n",
        "        accumulated_gradients = [\n",
        "            acc + grad / accumulation_steps\n",
        "            for acc, grad in zip(accumulated_gradients, gradients)\n",
        "        ]\n",
        "\n",
        "    # Apply accumulated gradients\n",
        "    optimizer.apply_gradients(zip(accumulated_gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nGradient Accumulation Pattern:\")\n",
        "print(\"  1. Compute gradients for mini-batch\")\n",
        "print(\"  2. Accumulate (sum or average) over N steps\")\n",
        "print(\"  3. Apply accumulated gradients once\")\n",
        "print(\"  4. Effective batch = mini_batch * N\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCMnLwcWcC9"
      },
      "source": [
        "---\n",
        "\n",
        "# Part II: Building Operations from Scratch\n",
        "\n",
        "## Understanding Neural Network Primitives\n",
        "\n",
        "Before using Keras layers, let's understand what they do by building them ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY-nVGvvWcC-",
        "outputId": "ed4fb8a2-e50a-499c-fe80-be0714415a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           CONVOLUTION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape:  (1, 5, 5, 1)\n",
            "Kernel shape: (3, 3, 1, 2)\n",
            "Output shape: (1, 3, 3, 2)\n",
            "Matches tf.nn.conv2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    CONVOLUTION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           CONVOLUTION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def conv2d_naive(input_tensor, kernel, stride=1, padding='VALID'):\n",
        "    \"\"\"\n",
        "    Naive 2D convolution implementation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_tensor : tensor (batch, height, width, in_channels)\n",
        "    kernel : tensor (kernel_h, kernel_w, in_channels, out_channels)\n",
        "    stride : int\n",
        "    padding : 'VALID' or 'SAME'\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    k_h, k_w = kernel.shape[0], kernel.shape[1]\n",
        "    out_channels = kernel.shape[3]\n",
        "\n",
        "    if padding == 'SAME':\n",
        "        pad_h = k_h // 2\n",
        "        pad_w = k_w // 2\n",
        "        input_tensor = tf.pad(input_tensor,\n",
        "                              [[0, 0], [pad_h, pad_h], [pad_w, pad_w], [0, 0]])\n",
        "        in_h += 2 * pad_h\n",
        "        in_w += 2 * pad_w\n",
        "\n",
        "    out_h = (in_h - k_h) // stride + 1\n",
        "    out_w = (in_w - k_w) // stride + 1\n",
        "\n",
        "    output = tf.TensorArray(dtype=tf.float32, size=out_h * out_w)\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            # Extract patch\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            patch = input_tensor[:, h_start:h_start+k_h, w_start:w_start+k_w, :]\n",
        "\n",
        "            # Convolve: sum over (h, w, in_channels), keep out_channels\n",
        "            # patch: (batch, k_h, k_w, in_c)\n",
        "            # kernel: (k_h, k_w, in_c, out_c)\n",
        "            conv = tf.einsum('bhwi,hwio->bo', patch, kernel)\n",
        "            output = output.write(idx, conv)\n",
        "            idx += 1\n",
        "\n",
        "    output = output.stack()  # (out_h*out_w, batch, out_c)\n",
        "    output = tf.transpose(output, [1, 0, 2])  # (batch, out_h*out_w, out_c)\n",
        "    output = tf.reshape(output, [batch_size, out_h, out_w, out_channels])\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test our implementation\n",
        "x = tf.random.normal((1, 5, 5, 1))  # 1 image, 5x5, 1 channel\n",
        "kernel = tf.random.normal((3, 3, 1, 2))  # 3x3 kernel, 1->2 channels\n",
        "\n",
        "our_output = conv2d_naive(x, kernel, stride=1, padding='VALID')\n",
        "tf_output = tf.nn.conv2d(x, kernel, strides=1, padding='VALID')\n",
        "\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Kernel shape: {kernel.shape}\")\n",
        "print(f\"Output shape: {our_output.shape}\")\n",
        "print(f\"Matches tf.nn.conv2d: {tf.reduce_all(tf.abs(our_output - tf_output) < 1e-5).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltjjl1lJWcC-",
        "outputId": "05e7d761-394c-4f56-fe31-be1daabebab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           MAX POOLING FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (1, 4, 4, 2)\n",
            "Input (channel 0):\n",
            "[[ 1.  3.  5.  7.]\n",
            " [ 9. 11. 13. 15.]\n",
            " [17. 19. 21. 23.]\n",
            " [25. 27. 29. 31.]]\n",
            "\n",
            "Output shape: (1, 2, 2, 2)\n",
            "Output (channel 0):\n",
            "[[11. 15.]\n",
            " [27. 31.]]\n",
            "\n",
            "Matches tf.nn.max_pool2d: True\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    MAX POOLING FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           MAX POOLING FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def max_pool2d_naive(input_tensor, pool_size=2, stride=2):\n",
        "    \"\"\"\n",
        "    Naive max pooling implementation.\n",
        "\n",
        "    For each pool_size x pool_size window, take the maximum.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(input_tensor)[0]\n",
        "    in_h, in_w, channels = input_tensor.shape[1:]\n",
        "\n",
        "    out_h = (in_h - pool_size) // stride + 1\n",
        "    out_w = (in_w - pool_size) // stride + 1\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for i in range(out_h):\n",
        "        row = []\n",
        "        for j in range(out_w):\n",
        "            h_start = i * stride\n",
        "            w_start = j * stride\n",
        "            # Extract window\n",
        "            window = input_tensor[:, h_start:h_start+pool_size,\n",
        "                                  w_start:w_start+pool_size, :]\n",
        "            # Max over spatial dimensions\n",
        "            pooled = tf.reduce_max(window, axis=[1, 2])\n",
        "            row.append(pooled)\n",
        "        outputs.append(tf.stack(row, axis=1))\n",
        "\n",
        "    return tf.stack(outputs, axis=1)\n",
        "\n",
        "# Test\n",
        "x = tf.constant([[[[1., 2.], [3., 4.], [5., 6.], [7., 8.]],\n",
        "                  [[9., 10.], [11., 12.], [13., 14.], [15., 16.]],\n",
        "                  [[17., 18.], [19., 20.], [21., 22.], [23., 24.]],\n",
        "                  [[25., 26.], [27., 28.], [29., 30.], [31., 32.]]]])\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input (channel 0):\")\n",
        "print(x[0, :, :, 0].numpy())\n",
        "\n",
        "our_pool = max_pool2d_naive(x, pool_size=2, stride=2)\n",
        "tf_pool = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='VALID')\n",
        "\n",
        "print(f\"\\nOutput shape: {our_pool.shape}\")\n",
        "print(f\"Output (channel 0):\")\n",
        "print(our_pool[0, :, :, 0].numpy())\n",
        "print(f\"\\nMatches tf.nn.max_pool2d: {tf.reduce_all(our_pool == tf_pool).numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeZuo5ITWcC-",
        "outputId": "0df0613e-e4cb-44a5-b88f-0f8040c280d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        BATCH NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (8, 4)\n",
            "Input mean per feature: [ 0.32574826 -0.22160122  0.37827352  0.3575499 ]\n",
            "Input std per feature:  [0.8102391  0.74234474 1.061351   1.0821929 ]\n",
            "\n",
            "Output (training) mean: [-2.9802322e-08  4.6566129e-08  7.4505806e-09 -2.2351742e-08]\n",
            "Output (training) std:  [0.9999923 0.9999908 0.9999955 0.9999958]\n",
            "\n",
            " After BatchNorm, each feature has ~0 mean and ~1 std!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    BATCH NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        BATCH NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class BatchNormFromScratch:\n",
        "    \"\"\"\n",
        "    Batch Normalization implemented from scratch.\n",
        "\n",
        "    During training:\n",
        "        x_norm = (x - batch_mean) / sqrt(batch_var + epsilon)\n",
        "        y = gamma * x_norm + beta\n",
        "\n",
        "    During inference:\n",
        "        Use running mean and variance instead of batch statistics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
        "        self.epsilon = epsilon\n",
        "        self.momentum = momentum\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(num_features), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(num_features), name='beta')\n",
        "\n",
        "        # Running statistics (not trainable)\n",
        "        self.running_mean = tf.Variable(tf.zeros(num_features), trainable=False)\n",
        "        self.running_var = tf.Variable(tf.ones(num_features), trainable=False)\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        if training:\n",
        "            # Compute batch statistics\n",
        "            batch_mean = tf.reduce_mean(x, axis=0)\n",
        "            batch_var = tf.math.reduce_variance(x, axis=0)\n",
        "\n",
        "            # Update running statistics\n",
        "            self.running_mean.assign(\n",
        "                (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
        "            )\n",
        "            self.running_var.assign(\n",
        "                (1 - self.momentum) * self.running_var + self.momentum * batch_var\n",
        "            )\n",
        "\n",
        "            mean, var = batch_mean, batch_var\n",
        "        else:\n",
        "            mean, var = self.running_mean, self.running_var\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "bn = BatchNormFromScratch(num_features=4)\n",
        "x = tf.random.normal((8, 4))  # Batch of 8, 4 features\n",
        "\n",
        "y_train = bn(x, training=True)\n",
        "y_eval = bn(x, training=False)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"Input mean per feature: {tf.reduce_mean(x, axis=0).numpy()}\")\n",
        "print(f\"Input std per feature:  {tf.math.reduce_std(x, axis=0).numpy()}\")\n",
        "print(f\"\\nOutput (training) mean: {tf.reduce_mean(y_train, axis=0).numpy()}\")\n",
        "print(f\"Output (training) std:  {tf.math.reduce_std(y_train, axis=0).numpy()}\")\n",
        "print(f\"\\n After BatchNorm, each feature has ~0 mean and ~1 std!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR_REX3RWcC-",
        "outputId": "0f38b026-fe4a-4a09-a496-f15f6e47a3fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        LAYER NORMALIZATION FROM SCRATCH\n",
            "============================================================\n",
            "\n",
            "Input shape: (2, 3, 4)\n",
            "\n",
            "For sample [0, 0, :]:\n",
            "  Input:  [ 0.65648675 -0.4130517   0.33997506 -1.0056272 ]\n",
            "  Output: [ 1.1744822  -0.47392502  0.6866642  -1.3872216 ]\n",
            "  Output mean: -0.000000\n",
            "  Output std:  1.0000\n",
            "\n",
            " Key difference:\n",
            "  BatchNorm: normalize across batch (for each feature)\n",
            "  LayerNorm: normalize across features (for each sample)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    LAYER NORMALIZATION FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        LAYER NORMALIZATION FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class LayerNormFromScratch:\n",
        "    \"\"\"\n",
        "    Layer Normalization: Normalize across features (not batch).\n",
        "\n",
        "    Used in Transformers because:\n",
        "    - Works with any batch size (including 1)\n",
        "    - No running statistics needed\n",
        "    - Each sample normalized independently\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        self.epsilon = epsilon\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "        # Learnable parameters\n",
        "        self.gamma = tf.Variable(tf.ones(normalized_shape), name='gamma')\n",
        "        self.beta = tf.Variable(tf.zeros(normalized_shape), name='beta')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Compute statistics across last dimensions\n",
        "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "        var = tf.math.reduce_variance(x, axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize\n",
        "        x_norm = (x - mean) / tf.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Scale and shift\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Test\n",
        "ln = LayerNormFromScratch(normalized_shape=4)\n",
        "x = tf.random.normal((2, 3, 4))  # (batch, seq, features)\n",
        "\n",
        "y = ln(x)\n",
        "\n",
        "print(f\"\\nInput shape: {x.shape}\")\n",
        "print(f\"\\nFor sample [0, 0, :]:\")\n",
        "print(f\"  Input:  {x[0, 0, :].numpy()}\")\n",
        "print(f\"  Output: {y[0, 0, :].numpy()}\")\n",
        "print(f\"  Output mean: {tf.reduce_mean(y[0, 0, :]).numpy():.6f}\")\n",
        "print(f\"  Output std:  {tf.math.reduce_std(y[0, 0, :]).numpy():.4f}\")\n",
        "\n",
        "print(f\"\\n Key difference:\")\n",
        "print(f\"  BatchNorm: normalize across batch (for each feature)\")\n",
        "print(f\"  LayerNorm: normalize across features (for each sample)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Ok8o68WcC_",
        "outputId": "c8873b16-2166-4811-90a3-4618c478c6b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "            DROPOUT FROM SCRATCH\n",
            "============================================================\n",
            "Input: all ones, shape (2, 10)\n",
            "\n",
            "Dropout sample 1: [0. 2. 2. 0. 0. 2. 0. 2. 2. 0.]\n",
            "Dropout sample 2: [0. 2. 2. 2. 0. 0. 2. 2. 2. 0.]\n",
            "Dropout sample 3: [2. 0. 2. 2. 2. 0. 0. 2. 2. 2.]\n",
            "\n",
            "During inference (training=False):\n",
            "  Output: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "\n",
            "Average over 1000 samples: 1.0045 (should be ~1.0)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "#                    DROPOUT FROM SCRATCH\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"            DROPOUT FROM SCRATCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def dropout_from_scratch(x, rate=0.5, training=True):\n",
        "    \"\"\"\n",
        "    Dropout: Randomly zero out neurons during training.\n",
        "\n",
        "    Key insight: Scale by 1/(1-rate) during training so that\n",
        "    expected value remains the same during inference.\n",
        "    \"\"\"\n",
        "    if not training or rate == 0:\n",
        "        return x\n",
        "\n",
        "    # Create random mask\n",
        "    keep_prob = 1 - rate\n",
        "    mask = tf.cast(\n",
        "        tf.random.uniform(tf.shape(x)) < keep_prob,\n",
        "        dtype=x.dtype\n",
        "    )\n",
        "\n",
        "    # Apply mask and scale\n",
        "    return (x * mask) / keep_prob\n",
        "\n",
        "# Test\n",
        "x = tf.ones((2, 10))\n",
        "\n",
        "print(f\"Input: all ones, shape {x.shape}\")\n",
        "print(f\"\")\n",
        "\n",
        "# Multiple dropout samples\n",
        "for i in range(3):\n",
        "    dropped = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "    print(f\"Dropout sample {i+1}: {dropped[0].numpy()}\")\n",
        "\n",
        "print(f\"\\nDuring inference (training=False):\")\n",
        "print(f\"  Output: {dropout_from_scratch(x, rate=0.5, training=False)[0].numpy()}\")\n",
        "\n",
        "# Verify expected value is preserved\n",
        "samples = tf.stack([dropout_from_scratch(x, rate=0.5) for _ in range(1000)])\n",
        "print(f\"\\nAverage over 1000 samples: {tf.reduce_mean(samples).numpy():.4f} (should be ~1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part III: Custom Layers Using Primitives\n",
        "\n",
        "## Building Layers with tf.Variable Only\n",
        "\n",
        "Before using Keras's layer system, let's build fully functional layers using only basic TensorFlow operations. This shows exactly what happens under the hood."
      ],
      "metadata": {
        "id": "4XqEZxTtWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    DENSE LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          DENSE LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class DenseLayerPrimitive:\n",
        "    \"\"\"\n",
        "    Fully connected layer using only tf.Variable.\n",
        "\n",
        "    Mathematically: y = activation(x @ W + b)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, activation=None, use_bias=True):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "            'softmax': lambda x: tf.nn.softmax(x, axis=-1)\n",
        "        }.get(activation, activation)  # Allow passing functions directly\n",
        "\n",
        "        # He initialization for weights\n",
        "        stddev = np.sqrt(2.0 / in_features)\n",
        "        self.W = tf.Variable(\n",
        "            tf.random.normal((in_features, out_features), stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.b = tf.Variable(\n",
        "                tf.zeros(out_features),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass: y = activation(x @ W + b)\"\"\"\n",
        "        out = x @ self.W\n",
        "        if self.use_bias:\n",
        "            out = out + self.b\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.W, self.b]\n",
        "        return [self.W]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"DenseLayerPrimitive({self.in_features}, {self.out_features})\"\n",
        "\n",
        "# Test\n",
        "dense = DenseLayerPrimitive(4, 3, activation='relu')\n",
        "x = tf.random.normal((2, 4))\n",
        "y = dense(x)\n",
        "\n",
        "print(f\"\\nDenseLayerPrimitive(4, 3, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Weight shape: {dense.W.shape}\")\n",
        "print(f\"Bias shape:   {dense.b.shape}\")\n",
        "print(f\"\\nOutput:\\n{y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gesc0NBxWcC_",
        "outputId": "eb7e800b-6231-476c-a08b-09d064f80fe0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          DENSE LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "DenseLayerPrimitive(4, 3, activation='relu')\n",
            "Input shape:  (2, 4)\n",
            "Output shape: (2, 3)\n",
            "Weight shape: (4, 3)\n",
            "Bias shape:   (3,)\n",
            "\n",
            "Output:\n",
            "[[0.         0.         0.46504724]\n",
            " [0.         0.         0.8249154 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CONV2D LAYER FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CONV2D LAYER FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class Conv2DLayerPrimitive:\n",
        "    \"\"\"\n",
        "    2D Convolutional layer using only tf.Variable and tf.nn.conv2d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding='SAME', activation=None, use_bias=True):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "        # Handle kernel_size as int or tuple\n",
        "        if isinstance(kernel_size, int):\n",
        "            kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "        # Select activation\n",
        "        self.activation = {\n",
        "            None: lambda x: x,\n",
        "            'relu': tf.nn.relu,\n",
        "            'sigmoid': tf.nn.sigmoid,\n",
        "            'tanh': tf.nn.tanh,\n",
        "        }.get(activation, activation)\n",
        "\n",
        "        # He initialization\n",
        "        fan_in = kernel_size[0] * kernel_size[1] * in_channels\n",
        "        stddev = np.sqrt(2.0 / fan_in)\n",
        "\n",
        "        # Kernel shape: (height, width, in_channels, out_channels)\n",
        "        self.kernel = tf.Variable(\n",
        "            tf.random.normal((kernel_size[0], kernel_size[1], in_channels, out_channels),\n",
        "                           stddev=stddev),\n",
        "            trainable=True,\n",
        "            name='kernel'\n",
        "        )\n",
        "\n",
        "        if use_bias:\n",
        "            self.bias = tf.Variable(\n",
        "                tf.zeros(out_channels),\n",
        "                trainable=True,\n",
        "                name='bias'\n",
        "            )\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"Forward pass using tf.nn.conv2d\"\"\"\n",
        "        out = tf.nn.conv2d(x, self.kernel, strides=self.stride, padding=self.padding)\n",
        "        if self.use_bias:\n",
        "            out = out + self.bias\n",
        "        return self.activation(out)\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if self.use_bias:\n",
        "            return [self.kernel, self.bias]\n",
        "        return [self.kernel]\n",
        "\n",
        "# Test\n",
        "conv = Conv2DLayerPrimitive(3, 16, kernel_size=3, activation='relu')\n",
        "x = tf.random.normal((1, 28, 28, 3))  # 1 image, 28x28, 3 channels\n",
        "y = conv(x)\n",
        "\n",
        "print(f\"\\nConv2DLayerPrimitive(3, 16, kernel_size=3)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {conv.kernel.shape}\")\n",
        "print(f\"Parameters:   {np.prod(conv.kernel.shape) + conv.bias.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSWj-SXwWcC_",
        "outputId": "afa147c3-15a0-4a6d-92ff-071a6e92c590"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CONV2D LAYER FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "Conv2DLayerPrimitive(3, 16, kernel_size=3)\n",
            "Input shape:  (1, 28, 28, 3)\n",
            "Output shape: (1, 28, 28, 16)\n",
            "Kernel shape: (3, 3, 3, 16)\n",
            "Parameters:   448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    COMPLETE CNN FROM PRIMITIVES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          COMPLETE CNN FROM PRIMITIVES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CNNFromPrimitives:\n",
        "    \"\"\"\n",
        "    A complete CNN built using only primitive layers.\n",
        "\n",
        "    Architecture:\n",
        "        Conv(3x3) -> ReLU -> MaxPool -> Conv(3x3) -> ReLU -> MaxPool -> Flatten -> Dense -> Dense\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        in_channels = input_shape[-1]\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = Conv2DLayerPrimitive(in_channels, 32, kernel_size=3, activation='relu')\n",
        "        self.conv2 = Conv2DLayerPrimitive(32, 64, kernel_size=3, activation='relu')\n",
        "\n",
        "        # Calculate flattened size after convolutions and pooling\n",
        "        # With SAME padding and 2x2 pooling twice: H/4, W/4\n",
        "        h, w = input_shape[0] // 4, input_shape[1] // 4\n",
        "        flat_size = h * w * 64\n",
        "\n",
        "        # Dense layers\n",
        "        self.fc1 = DenseLayerPrimitive(flat_size, 128, activation='relu')\n",
        "        self.fc2 = DenseLayerPrimitive(128, num_classes, activation='softmax')\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.fc1, self.fc2]\n",
        "\n",
        "    def __call__(self, x, training=True):\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
        "\n",
        "        # Flatten\n",
        "        x = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "\n",
        "        # Dense layers\n",
        "        x = self.fc1(x)\n",
        "        if training:\n",
        "            x = dropout_from_scratch(x, rate=0.5, training=True)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        variables = []\n",
        "        for layer in self.layers:\n",
        "            variables.extend(layer.trainable_variables)\n",
        "        return variables\n",
        "\n",
        "    def summary(self):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"        CNN FROM PRIMITIVES - SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        total = 0\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            params = sum(np.prod(v.shape) for v in layer.trainable_variables)\n",
        "            total += params\n",
        "            print(f\"Layer {i+1}: {layer.__class__.__name__:20} | Params: {params:,}\")\n",
        "        print(\"-\"*50)\n",
        "        print(f\"Total trainable parameters: {total:,}\")\n",
        "\n",
        "# Create and test\n",
        "cnn = CNNFromPrimitives(input_shape=(28, 28, 1), num_classes=10)\n",
        "cnn.summary()\n",
        "\n",
        "# Test forward pass\n",
        "x = tf.random.normal((4, 28, 28, 1))\n",
        "y = cnn(x)\n",
        "print(f\"\\nInput shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Output sum per sample: {tf.reduce_sum(y, axis=1).numpy()}  (should be ~1.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvCl_LZwWcC_",
        "outputId": "db46999a-bbfc-412f-8521-1ab45a63f424"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          COMPLETE CNN FROM PRIMITIVES\n",
            "============================================================\n",
            "\n",
            "==================================================\n",
            "        CNN FROM PRIMITIVES - SUMMARY\n",
            "==================================================\n",
            "Layer 1: Conv2DLayerPrimitive | Params: 320\n",
            "Layer 2: Conv2DLayerPrimitive | Params: 18,496\n",
            "Layer 3: DenseLayerPrimitive  | Params: 401,536\n",
            "Layer 4: DenseLayerPrimitive  | Params: 1,290\n",
            "--------------------------------------------------\n",
            "Total trainable parameters: 421,642\n",
            "\n",
            "Input shape:  (4, 28, 28, 1)\n",
            "Output shape: (4, 10)\n",
            "Output sum per sample: [0.9999999 1.        1.        0.9999999]  (should be ~1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part IV: Custom Keras Layers\n",
        "\n",
        "## The Proper Way to Build Custom Layers\n",
        "\n",
        "Keras provides a clean API for custom layers with:\n",
        "- **`build()`**: Create weights when input shape is known\n",
        "- **`call()`**: Define the forward pass\n",
        "- **`get_config()`**: Enable serialization\n",
        "\n",
        "This gives you all the benefits of primitives PLUS:\n",
        "- Automatic weight tracking\n",
        "- Serialization/deserialization\n",
        "- Integration with `model.fit()`\n",
        "- Proper shape inference"
      ],
      "metadata": {
        "id": "CPUeDPKIWcC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM KERAS LAYER: BASICS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          CUSTOM KERAS LAYER: BASICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomDenseLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom Dense layer demonstrating the Keras layer API.\n",
        "\n",
        "    Key methods:\n",
        "    - __init__: Store configuration (no weights yet!)\n",
        "    - build: Create weights when input shape is known\n",
        "    - call: Forward pass\n",
        "    - get_config: For serialization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, activation=None, use_bias=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.use_bias = use_bias\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        Create weights. Called automatically the first time the layer is used.\n",
        "\n",
        "        self.add_weight() creates a tf.Variable and registers it properly.\n",
        "        \"\"\"\n",
        "        self.kernel = self.add_weight(\n",
        "            name='kernel',\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',  # Xavier initialization\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        if self.use_bias:\n",
        "            self.bias = self.add_weight(\n",
        "                name='bias',\n",
        "                shape=(self.units,),\n",
        "                initializer='zeros',\n",
        "                trainable=True\n",
        "            )\n",
        "\n",
        "        # Mark as built\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        output = tf.matmul(inputs, self.kernel)\n",
        "        if self.use_bias:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Enable serialization.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'units': self.units,\n",
        "            'activation': keras.activations.serialize(self.activation),\n",
        "            'use_bias': self.use_bias\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test custom layer\n",
        "custom_dense = CustomDenseLayer(32, activation='relu')\n",
        "x = tf.random.normal((4, 16))\n",
        "y = custom_dense(x)  # This triggers build()\n",
        "\n",
        "print(f\"\\nCustomDenseLayer(32, activation='relu')\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Kernel shape: {custom_dense.kernel.shape}\")\n",
        "print(f\"Trainable variables: {len(custom_dense.trainable_variables)}\")\n",
        "print(f\"\\nConfig: {custom_dense.get_config()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnKJDd05WcC_",
        "outputId": "12b62341-ab04-41bb-a75f-6b889e21629b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          CUSTOM KERAS LAYER: BASICS\n",
            "============================================================\n",
            "\n",
            "CustomDenseLayer(32, activation='relu')\n",
            "Input shape:  (4, 16)\n",
            "Output shape: (4, 32)\n",
            "Kernel shape: (16, 32)\n",
            "Trainable variables: 2\n",
            "\n",
            "Config: {'name': 'custom_dense_layer', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'units': 32, 'activation': 'relu', 'use_bias': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SELF-ATTENTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         CUSTOM LAYER: SELF-ATTENTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SelfAttentionLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Self-Attention layer (simplified version of Transformer attention).\n",
        "\n",
        "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
        "\n",
        "    In self-attention, Q, K, V all come from the same input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Linear projections for Q, K, V\n",
        "        self.W_q = self.add_weight(\n",
        "            name='W_q',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_k = self.add_weight(\n",
        "            name='W_k',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_v = self.add_weight(\n",
        "            name='W_v',\n",
        "            shape=(input_shape[-1], self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        self.W_o = self.add_weight(\n",
        "            name='W_o',\n",
        "            shape=(self.embed_dim, self.embed_dim),\n",
        "            initializer='glorot_uniform'\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Linear projections\n",
        "        Q = inputs @ self.W_q  # (batch, seq, embed)\n",
        "        K = inputs @ self.W_k\n",
        "        V = inputs @ self.W_v\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = tf.reshape(Q, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        K = tf.reshape(K, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "        V = tf.reshape(V, (batch_size, seq_len, self.num_heads, self.head_dim))\n",
        "\n",
        "        # Transpose to (batch, heads, seq, head_dim)\n",
        "        Q = tf.transpose(Q, [0, 2, 1, 3])\n",
        "        K = tf.transpose(K, [0, 2, 1, 3])\n",
        "        V = tf.transpose(V, [0, 2, 1, 3])\n",
        "\n",
        "        # Attention scores: Q @ K^T / sqrt(d_k)\n",
        "        scale = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
        "        scores = tf.matmul(Q, K, transpose_b=True) / scale  # (batch, heads, seq, seq)\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores += (1 - mask) * -1e9\n",
        "\n",
        "        # Softmax\n",
        "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = tf.matmul(attention_weights, V)  # (batch, heads, seq, head_dim)\n",
        "\n",
        "        # Reshape back\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])  # (batch, seq, heads, head_dim)\n",
        "        context = tf.reshape(context, (batch_size, seq_len, self.embed_dim))\n",
        "\n",
        "        # Output projection\n",
        "        output = context @ self.W_o\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads\n",
        "        })\n",
        "        return config\n",
        "\n",
        "# Test\n",
        "attention = SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
        "x = tf.random.normal((2, 10, 64))  # (batch, seq_len, embed_dim)\n",
        "output, weights = attention(x)\n",
        "\n",
        "print(f\"\\nSelfAttentionLayer(embed_dim=64, num_heads=4)\")\n",
        "print(f\"Input shape:            {x.shape}\")\n",
        "print(f\"Output shape:           {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Trainable parameters:   {sum(np.prod(v.shape) for v in attention.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfKOP_WWWcDA",
        "outputId": "db38d9b0-04fe-46f9-ff3b-4a52072d592a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         CUSTOM LAYER: SELF-ATTENTION\n",
            "============================================================\n",
            "\n",
            "SelfAttentionLayer(embed_dim=64, num_heads=4)\n",
            "Input shape:            (2, 10, 64)\n",
            "Output shape:           (2, 10, 64)\n",
            "Attention weights shape: (2, 4, 10, 10)\n",
            "Trainable parameters:   16,384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"       CUSTOM LAYER: SPECTRAL NORMALIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SpectralNormalization(keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Spectral Normalization wrapper for layers.\n",
        "\n",
        "    Constrains the spectral norm (largest singular value) of the weight matrix.\n",
        "    Used in GANs to stabilize training.\n",
        "\n",
        "    W_normalized = W / sigma(W)\n",
        "    where sigma(W) is the largest singular value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, power_iterations=1, epsilon=1e-12, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        self.power_iterations = power_iterations\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.layer.build(input_shape)\n",
        "\n",
        "        # Get the weight matrix\n",
        "        self.w = self.layer.kernel\n",
        "        w_shape = self.w.shape.as_list()\n",
        "\n",
        "        # Flatten weight to 2D for SVD\n",
        "        self.w_flat_shape = (np.prod(w_shape[:-1]), w_shape[-1])\n",
        "\n",
        "        # Initialize u vector (for power iteration)\n",
        "        self.u = self.add_weight(\n",
        "            name='u',\n",
        "            shape=(1, w_shape[-1]),\n",
        "            initializer='random_normal',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        # Power iteration to estimate largest singular value\n",
        "        w_flat = tf.reshape(self.w, self.w_flat_shape)\n",
        "\n",
        "        u = self.u\n",
        "        for _ in range(self.power_iterations):\n",
        "            # v = W^T u / ||W^T u||\n",
        "            v = tf.matmul(u, tf.transpose(w_flat))\n",
        "            v = v / (tf.norm(v, ord='euclidean') + self.epsilon)\n",
        "\n",
        "            # u = W v / ||W v||\n",
        "            u = tf.matmul(v, w_flat)\n",
        "            u = u / (tf.norm(u, ord='euclidean') + self.epsilon)\n",
        "\n",
        "        if training:\n",
        "            self.u.assign(u)\n",
        "\n",
        "        # Spectral norm: sigma = u^T W v\n",
        "        # Corrected calculation for row vectors u (1, out_features) and v (1, in_features)\n",
        "        # The formula should be v W u^T\n",
        "        sigma = tf.matmul(tf.matmul(v, w_flat), tf.transpose(u))\n",
        "\n",
        "        # Normalize weight\n",
        "        w_normalized = self.w / sigma[0, 0]\n",
        "\n",
        "        # Manually perform the forward pass of the wrapped layer using w_normalized.\n",
        "        # This assumes the wrapped layer is a Dense layer based on the test case.\n",
        "        output = tf.matmul(inputs, w_normalized)\n",
        "\n",
        "        # If the wrapped layer has a bias and uses it, add it.\n",
        "        if hasattr(self.layer, 'bias') and self.layer.use_bias:\n",
        "            output = output + self.layer.bias\n",
        "\n",
        "        # If the wrapped layer has an activation function, apply it.\n",
        "        if hasattr(self.layer, 'activation') and self.layer.activation is not None:\n",
        "            output = self.layer.activation(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Test\n",
        "base_layer = layers.Dense(64)\n",
        "spectral_dense = SpectralNormalization(base_layer)\n",
        "x = tf.random.normal((4, 32))\n",
        "y = spectral_dense(x)\n",
        "\n",
        "print(f\"\\nSpectralNormalization(Dense(64))\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n Use case: Stabilize GAN discriminator training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFYq5xJbWcDA",
        "outputId": "c897aac7-3670-4edb-982d-0e3cd075b7bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "       CUSTOM LAYER: SPECTRAL NORMALIZATION\n",
            "============================================================\n",
            "\n",
            "SpectralNormalization(Dense(64))\n",
            "Input shape:  (4, 32)\n",
            "Output shape: (4, 64)\n",
            "\n",
            " Use case: Stabilize GAN discriminator training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part V: Advanced Architectures\n",
        "\n",
        "## Building Modern Deep Learning Components\n",
        "\n",
        "Now let's build advanced architectural components used in state-of-the-art models:\n",
        "\n",
        "- **Residual Blocks** (ResNet)\n",
        "- **Squeeze-and-Excitation** (SENet)\n",
        "- **Transformer Encoder Block**\n",
        "- **Custom Normalization Layers**"
      ],
      "metadata": {
        "id": "biOHWrOFWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    RESIDUAL BLOCK (ResNet Style)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"           RESIDUAL BLOCK (ResNet)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class ResidualBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Residual Block: y = F(x) + x\n",
        "\n",
        "    The key insight: learning residual F(x) = y - x is easier than learning y directly.\n",
        "    This enables training very deep networks (100+ layers).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filters, kernel_size=3, stride=1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        self.stride = stride\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = layers.Conv2D(filters, kernel_size, strides=stride,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "\n",
        "        self.conv2 = layers.Conv2D(filters, kernel_size, strides=1,\n",
        "                                    padding='same', use_bias=False)\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.skip_conv = None\n",
        "        self.skip_bn = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Need projection if dimensions change\n",
        "        if input_shape[-1] != self.filters or self.stride != 1:\n",
        "            self.skip_conv = layers.Conv2D(self.filters, 1, strides=self.stride,\n",
        "                                           padding='same', use_bias=False)\n",
        "            self.skip_bn = layers.BatchNormalization()\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        # Main path\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.bn1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x, training=training)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.skip_conv is not None:\n",
        "            skip = self.skip_conv(inputs)\n",
        "            skip = self.skip_bn(skip, training=training)\n",
        "        else:\n",
        "            skip = inputs\n",
        "\n",
        "        # Add and activate\n",
        "        return tf.nn.relu(x + skip)\n",
        "\n",
        "# Test\n",
        "res_block = ResidualBlock(64, stride=1)\n",
        "x = tf.random.normal((2, 32, 32, 64))\n",
        "y = res_block(x)\n",
        "\n",
        "print(f\"\\nResidualBlock(64)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "\n",
        "# With downsampling\n",
        "res_block_down = ResidualBlock(128, stride=2)\n",
        "y_down = res_block_down(x)\n",
        "print(f\"\\nResidualBlock(128, stride=2)\")\n",
        "print(f\"Output shape: {y_down.shape}  (spatial dims halved, channels doubled)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqnNTvJTWcDA",
        "outputId": "25857082-995f-4dba-adc9-e52d012c7375"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "           RESIDUAL BLOCK (ResNet)\n",
            "============================================================\n",
            "\n",
            "ResidualBlock(64)\n",
            "Input shape:  (2, 32, 32, 64)\n",
            "Output shape: (2, 32, 32, 64)\n",
            "\n",
            "ResidualBlock(128, stride=2)\n",
            "Output shape: (2, 16, 16, 128)  (spatial dims halved, channels doubled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    SQUEEZE-AND-EXCITATION BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"         SQUEEZE-AND-EXCITATION BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class SqueezeExcitationBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Squeeze-and-Excitation (SE) Block.\n",
        "\n",
        "    Learns channel-wise attention weights:\n",
        "    1. Squeeze: Global average pooling (H,W,C) -> (1,1,C)\n",
        "    2. Excitation: FC -> ReLU -> FC -> Sigmoid\n",
        "    3. Scale: Multiply input by attention weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, reduction_ratio=16, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.reduction_ratio = reduction_ratio\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        channels = input_shape[-1]\n",
        "        reduced_channels = max(channels // self.reduction_ratio, 1)\n",
        "\n",
        "        self.fc1 = layers.Dense(reduced_channels, activation='relu')\n",
        "        self.fc2 = layers.Dense(channels, activation='sigmoid')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeezed = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)  # (B, 1, 1, C)\n",
        "\n",
        "        # Excitation\n",
        "        x = tf.reshape(squeezed, (tf.shape(inputs)[0], -1))  # (B, C)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = tf.reshape(x, (tf.shape(inputs)[0], 1, 1, -1))  # (B, 1, 1, C)\n",
        "\n",
        "        # Scale\n",
        "        return inputs * x\n",
        "\n",
        "# Test\n",
        "se_block = SqueezeExcitationBlock(reduction_ratio=16)\n",
        "x = tf.random.normal((2, 28, 28, 64))\n",
        "y = se_block(x)\n",
        "\n",
        "print(f\"\\nSqueezeExcitationBlock(reduction_ratio=16)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"\\n SE learns which channels are important for the task\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcm-_VhvWcDA",
        "outputId": "2c6f0e41-9fb2-4270-d5ea-cdc90039ec0f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "         SQUEEZE-AND-EXCITATION BLOCK\n",
            "============================================================\n",
            "\n",
            "SqueezeExcitationBlock(reduction_ratio=16)\n",
            "Input shape:  (2, 28, 28, 64)\n",
            "Output shape: (2, 28, 28, 64)\n",
            "\n",
            " SE learns which channels are important for the task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRANSFORMER ENCODER BLOCK\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          TRANSFORMER ENCODER BLOCK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class TransformerEncoderBlock(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Encoder Block.\n",
        "\n",
        "    Architecture:\n",
        "        x -> LayerNorm -> MultiHeadAttention -> + (residual) ->\n",
        "             LayerNorm -> FeedForward -> + (residual) -> output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=embed_dim // num_heads\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(ff_dim, activation='gelu'),\n",
        "            layers.Dropout(dropout_rate),\n",
        "            layers.Dense(embed_dim),\n",
        "            layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # Pre-norm architecture (more stable)\n",
        "        # Attention block\n",
        "        x = self.layernorm1(inputs)\n",
        "        attn_output = self.attention(x, x, attention_mask=mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        x = inputs + attn_output  # Residual connection\n",
        "\n",
        "        # Feed-forward block\n",
        "        ffn_input = self.layernorm2(x)\n",
        "        ffn_output = self.ffn(ffn_input, training=training)\n",
        "\n",
        "        return x + ffn_output  # Residual connection\n",
        "\n",
        "# Test\n",
        "transformer_block = TransformerEncoderBlock(\n",
        "    embed_dim=64,\n",
        "    num_heads=4,\n",
        "    ff_dim=256,\n",
        "    dropout_rate=0.1\n",
        ")\n",
        "\n",
        "x = tf.random.normal((2, 20, 64))  # (batch, seq_len, embed_dim)\n",
        "y = transformer_block(x)\n",
        "\n",
        "print(f\"\\nTransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\")\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {y.shape}\")\n",
        "print(f\"Parameters:   {sum(np.prod(v.shape) for v in transformer_block.trainable_variables):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I-38YHnWcDA",
        "outputId": "68670e3d-9f16-4f50-a380-a6b7647caa43"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          TRANSFORMER ENCODER BLOCK\n",
            "============================================================\n",
            "\n",
            "TransformerEncoderBlock(embed_dim=64, num_heads=4, ff_dim=256)\n",
            "Input shape:  (2, 20, 64)\n",
            "Output shape: (2, 20, 64)\n",
            "Parameters:   49,984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VI: Custom Training Loops\n",
        "\n",
        "## Full Control with GradientTape\n",
        "\n",
        "While `model.fit()` is convenient, custom training loops give you:\n",
        "\n",
        "- **Complete control** over the training process\n",
        "- **Custom metrics** and logging\n",
        "- **Complex training schemes** (GANs, reinforcement learning)\n",
        "- **Gradient manipulation** (clipping, accumulation)\n",
        "- **Multi-GPU/TPU** strategies"
      ],
      "metadata": {
        "id": "z13eQIQpWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    BASIC CUSTOM TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"          BASIC CUSTOM TRAINING LOOP\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def custom_training_loop(model, train_data, val_data, epochs, learning_rate=0.001):\n",
        "    \"\"\"\n",
        "    Complete custom training loop with GradientTape.\n",
        "    \"\"\"\n",
        "    optimizer = keras.optimizers.Adam(learning_rate)\n",
        "    loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    # Metrics\n",
        "    train_loss = keras.metrics.Mean()\n",
        "    train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "    val_loss = keras.metrics.Mean()\n",
        "    val_acc = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Reset metrics\n",
        "        train_loss.reset_state()\n",
        "        train_acc.reset_state()\n",
        "\n",
        "        # Training loop\n",
        "        for x_batch, y_batch in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Forward pass\n",
        "                predictions = model(x_batch, training=True)\n",
        "                loss = loss_fn(y_batch, predictions)\n",
        "\n",
        "            # Backward pass\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "            # Update weights\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            # Update metrics\n",
        "            train_loss.update_state(loss)\n",
        "            train_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Validation loop\n",
        "        val_loss.reset_state()\n",
        "        val_acc.reset_state()\n",
        "\n",
        "        for x_batch, y_batch in val_data:\n",
        "            predictions = model(x_batch, training=False)\n",
        "            loss = loss_fn(y_batch, predictions)\n",
        "            val_loss.update_state(loss)\n",
        "            val_acc.update_state(y_batch, predictions)\n",
        "\n",
        "        # Record history\n",
        "        history['train_loss'].append(train_loss.result().numpy())\n",
        "        history['train_acc'].append(train_acc.result().numpy())\n",
        "        history['val_loss'].append(val_loss.result().numpy())\n",
        "        history['val_acc'].append(val_acc.result().numpy())\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {train_loss.result():.4f} | \"\n",
        "              f\"Train Acc: {train_acc.result():.4f} | \"\n",
        "              f\"Val Loss: {val_loss.result():.4f} | \"\n",
        "              f\"Val Acc: {val_acc.result():.4f}\")\n",
        "\n",
        "    return history\n",
        "\n",
        "print(\"\\nCustom training loop template created!\")\n",
        "print(\"Key components:\")\n",
        "print(\"  1. GradientTape for computing gradients\")\n",
        "print(\"  2. optimizer.apply_gradients() for weight updates\")\n",
        "print(\"  3. Metrics for tracking performance\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7sZ5woWcDA",
        "outputId": "f6926c6a-fc3e-4324-a2cc-9cf640bfe419"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "          BASIC CUSTOM TRAINING LOOP\n",
            "============================================================\n",
            "\n",
            "Custom training loop template created!\n",
            "Key components:\n",
            "  1. GradientTape for computing gradients\n",
            "  2. optimizer.apply_gradients() for weight updates\n",
            "  3. Metrics for tracking performance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    TRAINING WITH GRADIENT CLIPPING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        TRAINING WITH GRADIENT CLIPPING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "@tf.function\n",
        "def train_step_with_clipping(model, x, y, optimizer, loss_fn, clip_norm=1.0):\n",
        "    \"\"\"\n",
        "    Single training step with gradient clipping.\n",
        "\n",
        "    Gradient clipping prevents exploding gradients in deep networks.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x, training=True)\n",
        "        loss = loss_fn(y, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Clip gradients by global norm\n",
        "    gradients, global_norm = tf.clip_by_global_norm(gradients, clip_norm)\n",
        "\n",
        "    # Apply clipped gradients\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, global_norm\n",
        "\n",
        "print(\"Gradient Clipping Options:\")\n",
        "print(\"  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\")\n",
        "print(\"  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\")\n",
        "print(\"  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\")\n",
        "print(\"\\nGlobal norm is most common - maintains gradient direction!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt4KslvnWcDA",
        "outputId": "73a395b5-5bf7-4ee2-eda8-cd1086a46ed4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        TRAINING WITH GRADIENT CLIPPING\n",
            "============================================================\n",
            "Gradient Clipping Options:\n",
            "  - tf.clip_by_value(g, -clip, clip)  : Clip element-wise\n",
            "  - tf.clip_by_norm(g, clip)          : Clip each tensor by L2 norm\n",
            "  - tf.clip_by_global_norm(grads, clip): Clip all gradients together\n",
            "\n",
            "Global norm is most common - maintains gradient direction!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    CUSTOM MODEL WITH train_step()\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"        CUSTOM MODEL WITH train_step()\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "class CustomTrainableModel(keras.Model):\n",
        "    \"\"\"\n",
        "    Model with custom training logic built-in.\n",
        "\n",
        "    Override train_step() to customize what happens in model.fit().\n",
        "    This is the best of both worlds:\n",
        "    - Custom training logic\n",
        "    - Still use model.fit() with callbacks, validation, etc.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units_list, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.dense_layers = []\n",
        "        for units in units_list:\n",
        "            self.dense_layers.append(layers.Dense(units, activation='relu'))\n",
        "            self.dense_layers.append(layers.Dropout(0.2))\n",
        "\n",
        "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "        # Custom metrics\n",
        "        self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "        self.acc_tracker = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for layer in self.dense_layers:\n",
        "            x = layer(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"\n",
        "        Custom training step.\n",
        "        Called by model.fit() for each batch.\n",
        "        \"\"\"\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        # Compute and apply gradients\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "\n",
        "        # Update metrics\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Custom evaluation step.\"\"\"\n",
        "        x, y = data\n",
        "        y_pred = self(x, training=False)\n",
        "        loss = keras.losses.sparse_categorical_crossentropy(y, y_pred)\n",
        "\n",
        "        self.loss_tracker.update_state(tf.reduce_mean(loss))\n",
        "        self.acc_tracker.update_state(y, y_pred)\n",
        "\n",
        "        return {\n",
        "            'loss': self.loss_tracker.result(),\n",
        "            'accuracy': self.acc_tracker.result()\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n",
        "\n",
        "# Test\n",
        "custom_model = CustomTrainableModel([64, 32], num_classes=10)\n",
        "custom_model.compile(optimizer='adam')\n",
        "\n",
        "print(\"\\nCustom model with train_step() created!\")\n",
        "print(\"Now model.fit() uses our custom training logic\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ocCaxVeWcDA",
        "outputId": "1ef8c580-db6c-4064-dae6-fc68a711faa8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "        CUSTOM MODEL WITH train_step()\n",
            "============================================================\n",
            "\n",
            "Custom model with train_step() created!\n",
            "Now model.fit() uses our custom training logic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Part VII: Practical Demos\n",
        "\n",
        "## Putting It All Together\n",
        "\n",
        "Let's combine everything we've learned in real examples."
      ],
      "metadata": {
        "id": "S6cCw5HyWcDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#              DEMO 1: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load digits dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Preprocess\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for CNN: (samples, 8, 8, 1)\n",
        "X = X.reshape(-1, 8, 8, 1).astype(np.float32)\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples:     {X_test.shape[0]}\")\n",
        "print(f\"Image shape:      {X_train.shape[1:]}\")\n",
        "\n",
        "# Build custom ResNet model\n",
        "class MiniResNet(keras.Model):\n",
        "    \"\"\"Mini ResNet with custom residual blocks.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = layers.Conv2D(32, 3, padding='same', activation='relu')\n",
        "\n",
        "        # Residual blocks\n",
        "        self.res_block1 = ResidualBlock(32)\n",
        "        self.res_block2 = ResidualBlock(64, stride=2)\n",
        "\n",
        "        # SE block for channel attention\n",
        "        self.se_block = SqueezeExcitationBlock(reduction_ratio=8)\n",
        "\n",
        "        # Classification head\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.dropout = layers.Dropout(0.3)\n",
        "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        x = self.res_block1(x, training=training)\n",
        "        x = self.res_block2(x, training=training)\n",
        "        x = self.se_block(x)\n",
        "        x = self.global_pool(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create and compile\n",
        "tf.random.set_seed(42)\n",
        "resnet_model = MiniResNet(num_classes=10)\n",
        "\n",
        "resnet_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Build model by calling it\n",
        "_ = resnet_model(X_train[:1])\n",
        "resnet_model.summary()\n",
        "\n",
        "# Train\n",
        "print(\"\\nTraining MiniResNet...\")\n",
        "history = resnet_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = resnet_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hJng8MQqWcDA",
        "outputId": "d5ca887a-a2d2-4176-81db-2e3a350231f7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "     DEMO: CUSTOM RESNET FOR DIGIT CLASSIFICATION\n",
            "============================================================\n",
            "Training samples: 1437\n",
            "Test samples:     360\n",
            "Image shape:      (8, 8, 1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"mini_res_net\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"mini_res_net\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)                     \u001b[38;5;34m320\u001b[0m \n",
              "\n",
              " residual_block_2                 ?                              \u001b[38;5;34m18,688\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " residual_block_3                 ?                              \u001b[38;5;34m58,112\u001b[0m \n",
              " (\u001b[38;5;33mResidualBlock\u001b[0m)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               \u001b[38;5;34m1,096\u001b[0m \n",
              " (\u001b[38;5;33mSqueezeExcitationBlock\u001b[0m)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   \u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                               \n",
              "\n",
              " dropout_6 (\u001b[38;5;33mDropout\u001b[0m)              ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " dense_8 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m10\u001b[0m)                           \u001b[38;5;34m650\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n",
              "\n",
              " residual_block_2                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">18,688</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " residual_block_3                 ?                              <span style=\"color: #00af00; text-decoration-color: #00af00\">58,112</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                                                        \n",
              "\n",
              " squeeze_excitation_block_1       ?                               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,096</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SqueezeExcitationBlock</span>)                                               \n",
              "\n",
              " global_average_pooling2d         ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                               \n",
              "\n",
              " dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m78,866\u001b[0m (308.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,866</span> (308.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m78,354\u001b[0m (306.07 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,354</span> (306.07 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MiniResNet...\n",
            "Epoch 1/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - accuracy: 0.3711 - loss: 1.9688 - val_accuracy: 0.3139 - val_loss: 2.1822\n",
            "Epoch 2/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8773 - loss: 0.7323 - val_accuracy: 0.1833 - val_loss: 1.9720\n",
            "Epoch 3/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9525 - loss: 0.2678 - val_accuracy: 0.2250 - val_loss: 1.8296\n",
            "Epoch 4/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9907 - loss: 0.1187 - val_accuracy: 0.1778 - val_loss: 2.0763\n",
            "Epoch 5/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.9903 - loss: 0.0714 - val_accuracy: 0.2778 - val_loss: 1.8312\n",
            "Epoch 6/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9986 - loss: 0.0428 - val_accuracy: 0.3833 - val_loss: 1.5396\n",
            "Epoch 7/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9984 - loss: 0.0298 - val_accuracy: 0.7194 - val_loss: 0.7741\n",
            "Epoch 8/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9998 - loss: 0.0264 - val_accuracy: 0.9417 - val_loss: 0.1469\n",
            "Epoch 9/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9986 - loss: 0.0236 - val_accuracy: 0.9500 - val_loss: 0.1511\n",
            "Epoch 10/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9991 - loss: 0.0159 - val_accuracy: 0.9806 - val_loss: 0.0763\n",
            "Epoch 11/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0125 - val_accuracy: 0.9861 - val_loss: 0.0514\n",
            "Epoch 12/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.9833 - val_loss: 0.0507\n",
            "Epoch 13/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0081 - val_accuracy: 0.9889 - val_loss: 0.0411\n",
            "Epoch 14/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0104 - val_accuracy: 0.9917 - val_loss: 0.0261\n",
            "Epoch 15/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.9833 - val_loss: 0.0491\n",
            "Epoch 16/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.9917 - val_loss: 0.0363\n",
            "Epoch 17/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.9917 - val_loss: 0.0269\n",
            "Epoch 18/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.9889 - val_loss: 0.0329\n",
            "Epoch 19/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9917 - val_loss: 0.0303\n",
            "Epoch 20/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.9889 - val_loss: 0.0304\n",
            "Epoch 21/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.9889 - val_loss: 0.0386\n",
            "Epoch 22/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.9944 - val_loss: 0.0208\n",
            "Epoch 23/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9917 - val_loss: 0.0225\n",
            "Epoch 24/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.9889 - val_loss: 0.0274\n",
            "Epoch 25/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9917 - val_loss: 0.0237\n",
            "Epoch 26/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0023 - val_accuracy: 0.9944 - val_loss: 0.0175\n",
            "Epoch 27/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 0.0032 - val_accuracy: 0.9889 - val_loss: 0.0354\n",
            "Epoch 28/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.9917 - val_loss: 0.0221\n",
            "Epoch 29/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 1.0000 - loss: 0.0025 - val_accuracy: 0.9944 - val_loss: 0.0210\n",
            "Epoch 30/30\n",
            "\u001b[1m45/45\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9998 - loss: 0.0028 - val_accuracy: 0.7639 - val_loss: 1.2870\n",
            "\n",
            "Test Accuracy: 76.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                    VISUALIZE TRAINING RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "# Plot training history\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "ax1.plot(history.history['loss'], 'b-', label='Train Loss')\n",
        "ax1.plot(history.history['val_loss'], 'r-', label='Val Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('MiniResNet Training Loss', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(history.history['accuracy'], 'b-', label='Train Acc')\n",
        "ax2.plot(history.history['val_accuracy'], 'r-', label='Val Acc')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.set_title('MiniResNet Training Accuracy', fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "fig.suptitle('MiniResNet Predictions', fontsize=14, fontweight='bold')\n",
        "\n",
        "predictions = resnet_model.predict(X_test[:10], verbose=0)\n",
        "pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    img = X_test[i].reshape(8, 8)\n",
        "    true_label = y_test[i]\n",
        "    pred_label = pred_classes[i]\n",
        "    confidence = predictions[i][pred_label] * 100\n",
        "\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\n({confidence:.1f}%)',\n",
        "                 color=color, fontsize=10)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "6QkXz-l8WcDA",
        "outputId": "f3ff0a05-896c-4668-c407-ab1bf20502fe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "#                         ADVANCED CHEAT SHEET\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "cheat_sheet = \"\"\"\n",
        "GRADIENTTAPE PATTERNS\n",
        "---------------------\n",
        "# Basic gradient\n",
        "with tf.GradientTape() as tape:\n",
        "    y = model(x)\n",
        "grads = tape.gradient(y, model.trainable_variables)\n",
        "\n",
        "# Higher-order derivatives (nested tapes)\n",
        "with tf.GradientTape() as t2:\n",
        "    with tf.GradientTape() as t1:\n",
        "        y = f(x)\n",
        "    dy = t1.gradient(y, x)\n",
        "d2y = t2.gradient(dy, x)\n",
        "\n",
        "# Jacobian\n",
        "jacobian = tape.jacobian(y, x)\n",
        "\n",
        "# Custom gradient\n",
        "@tf.custom_gradient\n",
        "def custom_op(x):\n",
        "    def grad(dy):\n",
        "        return dy * custom_backward\n",
        "    return forward_result, grad\n",
        "\n",
        "CUSTOM KERAS LAYERS\n",
        "-------------------\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        return inputs @ self.kernel\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['units'] = self.units\n",
        "        return config\n",
        "\n",
        "CUSTOM TRAINING\n",
        "---------------\n",
        "# Override train_step for model.fit()\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)\n",
        "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "        grads = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
        "        return {'loss': loss}\n",
        "\n",
        "GRADIENT MANIPULATION\n",
        "---------------------\n",
        "# Clip by global norm\n",
        "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
        "\n",
        "# Gradient accumulation\n",
        "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
        "\"\"\"\n",
        "print(cheat_sheet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5Z_iXlFWcDA",
        "outputId": "e6481bea-f63b-46a9-af50-9291b67d429d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "              ADVANCED TENSORFLOW/KERAS CHEAT SHEET\n",
            "======================================================================\n",
            "\n",
            "GRADIENTTAPE PATTERNS\n",
            "---------------------\n",
            "# Basic gradient\n",
            "with tf.GradientTape() as tape:\n",
            "    y = model(x)\n",
            "grads = tape.gradient(y, model.trainable_variables)\n",
            "\n",
            "# Higher-order derivatives (nested tapes)\n",
            "with tf.GradientTape() as t2:\n",
            "    with tf.GradientTape() as t1:\n",
            "        y = f(x)\n",
            "    dy = t1.gradient(y, x)\n",
            "d2y = t2.gradient(dy, x)\n",
            "\n",
            "# Jacobian\n",
            "jacobian = tape.jacobian(y, x)\n",
            "\n",
            "# Custom gradient\n",
            "@tf.custom_gradient\n",
            "def custom_op(x):\n",
            "    def grad(dy):\n",
            "        return dy * custom_backward\n",
            "    return forward_result, grad\n",
            "\n",
            "CUSTOM KERAS LAYERS\n",
            "-------------------\n",
            "class CustomLayer(keras.layers.Layer):\n",
            "    def __init__(self, units, **kwargs):\n",
            "        super().__init__(**kwargs)\n",
            "        self.units = units\n",
            "\n",
            "    def build(self, input_shape):\n",
            "        self.kernel = self.add_weight(\n",
            "            shape=(input_shape[-1], self.units),\n",
            "            initializer='glorot_uniform',\n",
            "            trainable=True\n",
            "        )\n",
            "        super().build(input_shape)\n",
            "\n",
            "    def call(self, inputs, training=False):\n",
            "        return inputs @ self.kernel\n",
            "\n",
            "    def get_config(self):\n",
            "        config = super().get_config()\n",
            "        config['units'] = self.units\n",
            "        return config\n",
            "\n",
            "CUSTOM TRAINING\n",
            "---------------\n",
            "# Override train_step for model.fit()\n",
            "class CustomModel(keras.Model):\n",
            "    def train_step(self, data):\n",
            "        x, y = data\n",
            "        with tf.GradientTape() as tape:\n",
            "            y_pred = self(x, training=True)\n",
            "            loss = self.compute_loss(y=y, y_pred=y_pred)\n",
            "        grads = tape.gradient(loss, self.trainable_variables)\n",
            "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
            "        return {'loss': loss}\n",
            "\n",
            "GRADIENT MANIPULATION\n",
            "---------------------\n",
            "# Clip by global norm\n",
            "grads, _ = tf.clip_by_global_norm(grads, max_norm=1.0)\n",
            "\n",
            "# Gradient accumulation\n",
            "accumulated = [acc + g/steps for acc, g in zip(accumulated, grads)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "## Your Advanced TensorFlow & Keras Journey\n",
        "\n",
        "Congratulations! You've mastered advanced TensorFlow and Keras techniques.\n",
        "\n",
        "### What You Learned\n",
        "\n",
        "| Part | Topic | Key Takeaway |\n",
        "|------|-------|-------------|\n",
        "| I | Advanced GradientTape | Nested tapes, Jacobians, custom gradients |\n",
        "| II | Building Ops | Conv, pooling, normalization from scratch |\n",
        "| III | Primitive Layers | Dense, Conv2D with only tf.Variable |\n",
        "| IV | Custom Keras Layers | Proper subclassing with build() and call() |\n",
        "| V | Advanced Architectures | ResNet, SE-Net, Transformer blocks |\n",
        "| VI | Custom Training | Full control with GradientTape |\n",
        "| VII | Practical Demos | Real-world model combining everything |\n",
        "\n",
        "### When to Use What\n",
        "\n",
        "| Approach | Use When |\n",
        "|----------|----------|\n",
        "| `model.fit()` | Standard training, quick prototyping |\n",
        "| Custom `train_step()` | Custom logic but want callbacks/validation |\n",
        "| Full GradientTape loop | GANs, RL, complex multi-model training |\n",
        "| Custom layers | Reusable components, research |\n",
        "| Primitive layers | Learning, debugging, maximum control |\n",
        "\n",
        "### The Complete Learning Path\n",
        "\n",
        "1. **NumPy from Scratch** - Understand the math deeply\n",
        "2. **PyTorch** - Research-friendly framework\n",
        "3. **TensorFlow/Keras Part 1** - Fundamentals and high-level API\n",
        "4. **TensorFlow/Keras Part 2** - Advanced custom components (This notebook!)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- **Vision Transformers (ViT)** - Transformers for images\n",
        "- **Diffusion Models** - State-of-the-art generative AI\n",
        "- **Neural Architecture Search** - Automated model design\n",
        "- **Quantization & Pruning** - Model optimization for deployment\n",
        "- **TensorFlow Extended (TFX)** - Production ML pipelines\n",
        "\n",
        "---\n",
        "\n",
        "*\"The more you understand the primitives, the better you can innovate.\"*\n",
        "\n",
        "**Happy Deep Learning!**"
      ],
      "metadata": {
        "id": "wQs6OOHGWcDA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}